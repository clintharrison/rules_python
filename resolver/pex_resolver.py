from __future__ import absolute_import, print_function, unicode_literals

import argparse
import hashlib
import os
import re
from collections import defaultdict

from pex.fetcher import PyPIFetcher, Fetcher
from pex.interpreter import PythonInterpreter
from pex.package import WheelPackage
from pex.resolvable import Resolvable
from pex.resolver import _ResolvableSet, ResolvedDistribution, resolve_multi
from pex.requirements import requirements_from_file
from pex.resolver_options import ResolverOptionsBuilder
from pkg_resources import Distribution

if False:
    from typing import Dict, Optional, Set, List, Tuple


# Internally in pex.resolver.Resolver.resolve, a pex.resolver._ResolvableSet is used to store the
# list of packages that were successfully downloaded, and the version constraints on their dependencies.
# The resolver resolves in "round" of packages, until it stabilizes at a fully-solved transitive dependency
# graph. At the end of each round, the ResolvableSet has all dependencies specified as WheelPackages with a URL.
# It then calls a _ResolvableSet.replace_built() function which replaces the remote WheelPackages with their local
# equivalents.
# This global is used to save the original URLs and their local equivalents, so that we can reunite them
# once the resolver has converged.
local_dist_to_url = {}


REQUIREMENTS_BZL_TEMPLATE = """\
# AUTOGENERATED FILE. DO NOT MODIFY.

load("@bazel_tools//tools/build_defs/repo:http.bzl", "http_file")
load("@rules_python//python:whl.bzl", "whl_library")

def pip_install():
    _existing_rules = native.existing_rules()
{whl_libraries}

_requirements = {{
{mappings}
}}

def requirement(name):
    fail("requirement() has been renamed requirements(). It now takes and returns a list instead of a string.")

def requirements(names):
    if type(names) != "list":
        fail("requirements() now takes a list and returns a list")

    deps = []
    for name in names:
        name_key = name.replace("-", "_").lower()
        if name_key not in _requirements:
            fail("Could not find pip-provided dependency: '%s'" % name)
        deps += _requirements[name_key]
    return deps
"""

WHL_LIBRARY_TEMPLATE = """
    if "{download_repo}" not in _existing_rules:
        http_file(
            name = "{download_repo}",
            urls = ["{url}"],
            sha256 = "{sha256}",
            downloaded_file_path = "{filename}",
        )

    if "{repo_name}" not in _existing_rules:
        whl_library(
            name = "{repo_name}",
            whl = "@{download_repo}//file:{filename}",
            requirements = "{requirements_label}",
            extras = [{extras}],
            python_interpreter = "{interpreter}",
        )"""

# map a short platform name to a valid Bazel config_setting target
PLATFORM_MAP = {
    "linux": "@rules_python//platforms:setting_linux_x86_64",
    "osx": "@rules_python//platforms:setting_osx_x86_64",
}


def wheel_to_short_platform(wheel):  # type: (WheelPackage) -> unicode
    """
    Wheel platforms may have a variety of names, and this function simplifies them so we can use an equivalent
    Bazel platform.
    """
    platform = wheel._arch_tag  # type: unicode
    if platform == 'any':
        return 'any'
    elif platform.startswith('macosx'):
        return 'osx'
    elif platform.startswith('manylinux1') or platform.startswith('linux'):
        return 'linux'

    raise ValueError('Unknown tag {}'.format(platform))


def _replace_built(self, built_packages):
    """
    Wrap the real _ResolvableSet.replace_built function so that remote WheelPackage URLs get saved.
    """
    for remote_pkg, local_dist in built_packages.items():
        local_dist_to_url[local_dist] = remote_pkg
    return self.__replace_built(built_packages)


_ResolvableSet.__replace_built = _ResolvableSet.replace_built
_ResolvableSet.replace_built = _replace_built


def package_to_bazel_repository(prefix, package, py_version, platform):
    # type: (unicode, WheelPackage, unicode, unicode) -> unicode
    """
    Convert a WheelPackage to a valid Bazel repository name, ensuring it is globally unique when the same package
    is downloaded multiple times at differing package versions, Python versions, and platforms.
    """
    canonical = '{}__{}_{}__{}_{}'.format(prefix, package.name, package.version, py_version, platform)
    return re.sub('[-.+]', '_', canonical)


def is_extra_possible(dist, extra, pkgs_seen):
    # type: (Distribution, unicode, Set[unicode]) -> bool
    """
    A Distribution contains the metadata for a single wheel; verify that all the dependencies for the given extra
    were actually downloaded by the pex resolver. If a dependency is missing, that means the extra is not possible,
    and we exclude it to prevent import failures at runtime.
    """
    if not extra:
        return False

    for extra_req in dist.requires(extras=(extra,)):
        # TODO(clint): compare the extra's `Requirement` against the platform/interpreter used by the resolver
        if normalize_name(extra_req.project_name) not in pkgs_seen:
            return False

    return True


def get_valid_extras(dist, pkgs_seen):  # type: (Distribution, Set[unicode]) -> [unicode]
    return [extra for extra in dist.extras if is_extra_possible(dist, extra, pkgs_seen)]


class WheelDependency(object):
    def __init__(self, ):
        super(WheelDependency, self).__init__()


def normalize_name(name):
    return name.lower().replace('-', '_')


def main():
    parser = argparse.ArgumentParser()

    parser.add_argument('--no-index', action='store_true', help='')
    parser.add_argument('--find-links', action='append')
    parser.add_argument('--extra-index-url', action='append')
    parser.add_argument('--platform', required=True, action='append')
    parser.add_argument('--python_interpreter', required=True)
    parser.add_argument('--requirement', required=True)
    parser.add_argument('--requirements_label', required=True)

    args = parser.parse_args()
    find_links = args.find_links or []
    extra_index_urls = args.extra_index_url or []

    fetchers = []  # type: [Fetcher]

    for find_links_arg in find_links:
        fetchers.append(Fetcher([find_links_arg]))

    for index_url_arg in extra_index_urls:
        fetchers.append(PyPIFetcher(index_url_arg))

    if not args.no_index:
        fetchers.append(PyPIFetcher())

    builder = ResolverOptionsBuilder(fetchers=fetchers, precedence=(WheelPackage,))

    requirements = requirements_from_file(filename=args.requirement, builder=builder)  # type: [Resolvable]

    interpreter = PythonInterpreter.from_env(args.python_interpreter)

    # Resolve all dependencies, downloading them if necessary.
    resolved = list(resolve_multi(
        requirements=requirements,
        interpreters=[interpreter],
        platforms=args.platform,
        use_manylinux=True,
    ))  # type: [ResolvedDistribution]

    # Now that we've populated `local_dist_to_url` by resolving dependencies, create a map from package name
    # to (local, remote) URL tuples.
    pkg_name_to_urls = defaultdict(list)  # type: Dict[unicode, List[Tuple[WheelPackage, WheelPackage]]]
    for local_wheel, remote_wheel in local_dist_to_url.items():
        pkg_name = normalize_name(local_wheel.name)
        pkg_name_to_urls[pkg_name].append((local_wheel, remote_wheel))

    pkg_name_to_resolved_dists = defaultdict(set)
    for resolvable in resolved:
        pkg_name = normalize_name(resolvable.distribution.project_name)
        pkg_name_to_resolved_dists[pkg_name].add(resolvable)

    print_pip_install(pkg_name_to_resolved_dists, pkg_name_to_urls, args.requirements_label, interpreter)


def interpreter_to_str(python_interpreter, short=False):
    # type: (PythonInterpreter, bool) -> unicode
    major, minor, _ = python_interpreter.version
    if short:
        return 'py{}{}'.format(major, minor)
    else:
        return 'python{}.{}'.format(major, minor)


def print_pip_install(pkg_name_to_resolved_dists, pkg_name_to_urls, requirements_label, interpreter):
    # type: (Dict[unicode, Set[ResolvedDistribution]], Dict[unicode, List[Tuple[WheelPackage, WheelPackage]]], unicode, PythonInterpreter) -> None
    """
    For all packages in the resolved set:
        - Generate a `http_file` to download the wheel
        - Generate a a `whl_library` to unpack the wheel and create a `py_library`
        - Save the mapping from short package name to select()/repository name for the above `py_library`
    """
    downloaded_package_names = set(pkg_name_to_resolved_dists.keys())

    # list of all the templated `http_archive` and `whl_library` rules
    whl_libraries = []  # type: List[unicode]

    # {'psutil': {'linux': '@pypi__psutil_1_0__py2_linux//:pkg', 'osx': '@pypi__psutil_1_0__py2_linux//:pkg'},
    #  'pex':    {'any': '@pypi__pex_1_0__py2_any//:pkg'} }
    pkg_name_to_repo_names = defaultdict(dict)  # type: Dict[Tuple[unicode, unicode], Dict[unicode, unicode]]

    # Iterate over all downloaded packages in alphabetical order by package name.
    # These are sorted so the generated .bzl file changes minimally as packages are added and removed.
    for pkg_name in sorted(downloaded_package_names, key=lambda pkg: pkg):
        for local_wheel, remote_wheel in pkg_name_to_urls[pkg_name]:
            short_platform_name = wheel_to_short_platform(local_wheel)
            with open(local_wheel.local_path, 'rb') as whl:
                wheel_hash = hashlib.sha256(whl.read()).hexdigest()

            # unfortunately, we need a way to join the ResolvedDistribution with the WheelPackage;
            # this happens in very different steps of the pex resolver; to minimize the amount of
            # monkey patching we have to do, we do this ugly search instead.
            local_dist = None
            for dist in pkg_name_to_resolved_dists[pkg_name]:  # type: Distribution
                dist_filename = os.path.basename(dist.distribution.location)
                wheel_filename = os.path.basename(local_wheel.local_path)
                if dist_filename == wheel_filename:
                    local_dist = dist.distribution
                    break

            # not all extras in the wheel metadata are valid -- only include the extras whose dependencies
            # we have also downloaded.
            valid_extras = get_valid_extras(local_dist, downloaded_package_names)

            # We can share a single wheel if it has a py2.py3
            whl_repo_name = package_to_bazel_repository('whl', remote_wheel, local_wheel._py_tag, short_platform_name)
            # The repository name for a whl_library() needs to be py2 or py3 specific, so that the generated
            # py_library deps are correct. A py2.py3 wheel may have version-specific dependencies that must be
            # propagated to the underlying py_library.
            lib_repo_name = package_to_bazel_repository('pypi', remote_wheel, interpreter_to_str(interpreter, short=True), short_platform_name)

            # save the generated `http_file` and `whl_library` rules
            whl_libraries.append(WHL_LIBRARY_TEMPLATE.format(
                download_repo=whl_repo_name,
                url=remote_wheel.url,
                sha256=wheel_hash,
                filename=local_wheel.filename,
                repo_name=lib_repo_name,
                requirements_label=requirements_label,
                extras=", ".join('"{}"'.format(e) for e in valid_extras),
                interpreter=interpreter_to_str(interpreter),
            ))

            # save the mapping from package name to whl_library repository name, without an extra
            pkg_name_to_repo_names[(pkg_name, '')][short_platform_name] = lib_repo_name

            for extra in valid_extras:
                pkg_name_to_repo_names[(pkg_name, extra)][short_platform_name] = lib_repo_name

    # mappings is a list of strings containing the "short_name": select("...": "@repo") mapping
    # that underlies the convenience function requirement()
    mappings = []

    # Iterate over sorted package/repo dict by ordered ('package', 'extra')
    # This minimizes changes as individual packages change.
    for pkg_extra, repos in sorted(pkg_name_to_repo_names.items(), key=lambda item: item[0]):
        pkg_name, extra = pkg_extra
        if not extra:
            short_pkg_name = pkg_name
            py_lib_name = 'pkg'
        else:
            short_pkg_name = '{}[{}]'.format(pkg_name, extra)
            py_lib_name = extra

        # if we downloaded a wheel that's compatible with all platforms, use it and skip the select()
        # This must still return a list with a single item so it can be concatenated with a possible select(),
        # which for Starlark-type-checker implementation reasons cannot be used as an element of a string list.
        if 'any' in repos:
            mappings.append(
                '    "{pkg_name}": ["@{repo_name}//:{py_lib_name}"]'.format(
                    pkg_name=short_pkg_name,
                    repo_name=repos['any'],
                    py_lib_name=py_lib_name,
                )
            )
        else:
            mappings.append('\n'.join(
                ['    "{pkg_name}": select({{'.format(pkg_name=short_pkg_name)] +
                ['        "{platform}": ["@{repo_name}//:{py_lib_name}"],'.format(
                    repo_name=repos[platform],
                    platform=PLATFORM_MAP[platform],
                    py_lib_name=py_lib_name
                ) for platform in repos] +
                ['    })']
            ))

    print(REQUIREMENTS_BZL_TEMPLATE.format(
        whl_libraries='\n'.join(whl_libraries),
        # write a final comma to appease buildifier
        mappings=',\n'.join(mappings) + ','
    ))


if __name__ == '__main__':
    # When running this as a `bazel run @rules_python//...` command, it's convenient to use relative paths to files
    # inside the workspace, e.g. to build/common/requirements/requirements.2.0.txt. By changing directories
    # out of the execroot and back into the workspace root, this becomes possible.
    if 'BUILD_WORKSPACE_DIRECTORY' in os.environ:
        os.chdir(os.getenv('BUILD_WORKSPACE_DIRECTORY'))

    main()
